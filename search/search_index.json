{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AWS Controllers for Kubernetes (ACK) \u00b6 The AWS Controllers for Kubernetes (ACK) will allow containerized applications and Kubernetes users to create, update, delete and retrieve the status of resources in AWS services such as S3 buckets, DynamoDB, RDS databases, SNS, etc. using the Kubernetes API, for example using Kubernetes manifests or kubectl plugins. ACK comprises a set of Kubernetes custom controllers . Each controller manages custom resources representing API resources of a single AWS service API. For example, the service controller for AWS Simple Storage Service (S3) manages custom resources that represent AWS S3 buckets, keys, etc. Instead of logging into the AWS console or using the aws CLI tool to interact with the AWS service API, Kubernetes users can install a controller for an AWS service and then create, update, read and delete AWS resources using the Kubernetes API. This means they can use the Kubernetes API to fully describe both their containerized applications, using Kubernetes resources like Deployment and Service , as well as any AWS managed services upon which those applications depend. We are currently in the MVP phase, see also the issue 22 for details. If you have feedback, questions, or suggestions please don't hesitate to submit an issue , a pull request or comment on an existing issue.","title":"Home"},{"location":"#aws-controllers-for-kubernetes-ack","text":"The AWS Controllers for Kubernetes (ACK) will allow containerized applications and Kubernetes users to create, update, delete and retrieve the status of resources in AWS services such as S3 buckets, DynamoDB, RDS databases, SNS, etc. using the Kubernetes API, for example using Kubernetes manifests or kubectl plugins. ACK comprises a set of Kubernetes custom controllers . Each controller manages custom resources representing API resources of a single AWS service API. For example, the service controller for AWS Simple Storage Service (S3) manages custom resources that represent AWS S3 buckets, keys, etc. Instead of logging into the AWS console or using the aws CLI tool to interact with the AWS service API, Kubernetes users can install a controller for an AWS service and then create, update, read and delete AWS resources using the Kubernetes API. This means they can use the Kubernetes API to fully describe both their containerized applications, using Kubernetes resources like Deployment and Service , as well as any AWS managed services upon which those applications depend. We are currently in the MVP phase, see also the issue 22 for details. If you have feedback, questions, or suggestions please don't hesitate to submit an issue , a pull request or comment on an existing issue.","title":"AWS Controllers for Kubernetes (ACK)"},{"location":"community/background/","text":"Background \u00b6 In 10/2018 Chris Hein introduced the AWS Service Operator (ASO) project. We reviewed the feedback from the community and stakeholders and in 08/2019 decided to relaunch ASO as a first-tier open source project with concrete commitments from the container service team. In this process, we renamed the project to AWS Controllers for Kubernetes (ACK). The tenets for the relaunch were: ACK is a community-driven project, based on a governance model defining roles and responsibilities. ACK is optimized for production usage with full test coverage including performance and scalability test suites. ACK strives to be the only codebase exposing AWS services via a Kubernetes operator. Since then, we worked on design issues and gathering feedback around which services to prioritize. Existing custom controllers \u00b6 AWS service teams use custom controllers, webhooks, and operators for different use cases and based on different approaches. Examples include: Sagemaker operator , allowing to use Sagemaker from Kubernetes App Mesh controller , managing App Mesh resources from Kubernetes EKS Pod Identity Webhook , providing IAM roles for service accounts functionality While the autonomy in the different teams and project allows for rapid iterations and innovations, there are some drawbacks associated with it: The UX differs and that can lead to frustration when adopting an offering. A consistent quality bar across the different offerings is hard to establish and to verify. It's wasteful to re-invent the plumbing and necessary infrastructure (testing, etc.). Above is the motivation for our 3rd tenet: we want to make sure that there is a common framework, implementing good practices as put forward, for example, in the Operator Developer Guide or in the Programming Kubernetes book. Related projects \u00b6 Outside of AWS, there are projects that share similar goals we have with the ASO, for example: Crossplane aws-s3-provisioner","title":"Background"},{"location":"community/background/#background","text":"In 10/2018 Chris Hein introduced the AWS Service Operator (ASO) project. We reviewed the feedback from the community and stakeholders and in 08/2019 decided to relaunch ASO as a first-tier open source project with concrete commitments from the container service team. In this process, we renamed the project to AWS Controllers for Kubernetes (ACK). The tenets for the relaunch were: ACK is a community-driven project, based on a governance model defining roles and responsibilities. ACK is optimized for production usage with full test coverage including performance and scalability test suites. ACK strives to be the only codebase exposing AWS services via a Kubernetes operator. Since then, we worked on design issues and gathering feedback around which services to prioritize.","title":"Background"},{"location":"community/background/#existing-custom-controllers","text":"AWS service teams use custom controllers, webhooks, and operators for different use cases and based on different approaches. Examples include: Sagemaker operator , allowing to use Sagemaker from Kubernetes App Mesh controller , managing App Mesh resources from Kubernetes EKS Pod Identity Webhook , providing IAM roles for service accounts functionality While the autonomy in the different teams and project allows for rapid iterations and innovations, there are some drawbacks associated with it: The UX differs and that can lead to frustration when adopting an offering. A consistent quality bar across the different offerings is hard to establish and to verify. It's wasteful to re-invent the plumbing and necessary infrastructure (testing, etc.). Above is the motivation for our 3rd tenet: we want to make sure that there is a common framework, implementing good practices as put forward, for example, in the Operator Developer Guide or in the Programming Kubernetes book.","title":"Existing custom controllers"},{"location":"community/background/#related-projects","text":"Outside of AWS, there are projects that share similar goals we have with the ASO, for example: Crossplane aws-s3-provisioner","title":"Related projects"},{"location":"community/discussions/","text":"Discussions \u00b6 For discussions, please use the #provider-aws channel on the Kubernetes Slack community or the mailing list .","title":"Discussions"},{"location":"community/discussions/#discussions","text":"For discussions, please use the #provider-aws channel on the Kubernetes Slack community or the mailing list .","title":"Discussions"},{"location":"community/faq/","text":"Frequently Asked Questions (FAQ) \u00b6 Service Broker \u00b6 Question Does ACK replace the service broker ? Answer For the time being, people using the service broker should continue to use it and we're coordinating with the maintainers to provide a unified solution. The service broker project is also an AWS activity that, with the general shift of focus in the community from service broker to operators, can be considered less actively developed. There are a certain things around application lifecycle management that the service broker currently covers and which are at this juncture not yet covered by the scope of ACK, however we expect in the mid to long run that these two projects converge. We had AWS-internal discussions with the team that maintains the service broker and we're on the same page concerning a unified solution. We appreciate input and advice concerning features that are currently covered by the service broker only, for example bind/unbind or cataloging and looking forward to learn from the community how they are using service broker so that we can take this into account. Contributing \u00b6 Question Where and how can I help? Answer Excellent question and we're super excited that you're interested in ACK. For now, if you're a developer, you can check out the mvp branch and try out the code generation.","title":"FAQ"},{"location":"community/faq/#frequently-asked-questions-faq","text":"","title":"Frequently Asked Questions (FAQ)"},{"location":"community/faq/#service-broker","text":"Question Does ACK replace the service broker ? Answer For the time being, people using the service broker should continue to use it and we're coordinating with the maintainers to provide a unified solution. The service broker project is also an AWS activity that, with the general shift of focus in the community from service broker to operators, can be considered less actively developed. There are a certain things around application lifecycle management that the service broker currently covers and which are at this juncture not yet covered by the scope of ACK, however we expect in the mid to long run that these two projects converge. We had AWS-internal discussions with the team that maintains the service broker and we're on the same page concerning a unified solution. We appreciate input and advice concerning features that are currently covered by the service broker only, for example bind/unbind or cataloging and looking forward to learn from the community how they are using service broker so that we can take this into account.","title":"Service Broker"},{"location":"community/faq/#contributing","text":"Question Where and how can I help? Answer Excellent question and we're super excited that you're interested in ACK. For now, if you're a developer, you can check out the mvp branch and try out the code generation.","title":"Contributing"},{"location":"dev-docs/code-generation/","text":"Code generation \u00b6 In order to keep the code for all the service controllers consistent, we will use a strategy of generating the custom resource definitions and controller code stubs for new AWS services. Options \u00b6 To generate the CRDs and controller stub code, we investigated a number of options: home-grown custom code generator kudo kubebuilder a hybrid custom code generator + sigs.kubernetes.io/controller-tools (CR) The original AWS Service Operator used a custom-built generator that processed YAML manifests describing the AWS service and used templates to generate CRDs , the controller code itself and the Go types that represent the CRDs in memory. It's worth noting that the CRDs and the controller code that was generated by the original ASO was very tightly coupled to CloudFormation. In fact, the CRDs for individual AWS services like S3 or RDS were thin wrappers around CloudFormation stacks that described the object being operated upon. kudo is a platform for building Kubernetes Operators. It stores state in its own kudo.dev CRDs and allows users to define \"plans\" for a deployed application to deploy itself. We determined that kudo was not a particularly good fit for ASO for a couple reasons. First, we needed a way to generate CRDs in several API groups (s3.aws.com and iam.aws.com for example) and the ASO controller code isn't deploying an \"application\" that needs to have a controlled deployment plan. Instead, ASO is a controller that facilitates creation and management of various AWS service objects using Kubernetes CRD instances. kubebuilder is the recommended upstream tool for generating CRDs and controller stub code. It is a Go binary that creates the scaffolding for CRDs and controller Go code. It has support for multiple API groups (e.g. s3.amazonaws.com and dynamodb.amazonaws.com ) in a single code repository, so allows for sensible separation of code. Our final option was to build a hybrid custom code generator that used controller-runtime under the hood but allowed us to generate controller stub code for multiple API groups and place generated code in directories that represented Go best practices. This option gives us the flexibility to generate the files and content for multiple API groups but still stay within the recommended guardrails of the upstream Kubernetes community. Hybrid custom+controller-runtime \u00b6 This approach uses multiple phases of code generation. The first code generation phase consumes model information from a canonical source of truth about an AWS service and the objects and interfaces that service exposes and generates one or more files containing code that exposes Go types for those objects. These \"type files\" should be annotated with the marker and comments that will allow the core code generators and controller-gen to do its work. Once we have generated the type files, we need to generate basic scaffolding for consumers of those types. The upstream code-generator project contains generators for this scaffolding. We should be able to make a modified version of the upstream generate-groups.sh script to generate all the defaults, deepcopy, informers, listers and clientset code. Next, we will need to generate some skeleton code for the reconciling controller handling the new API along with the YAML files representing the CRDs that will get loaded into kube-apiserver and the YAML files for RBAC and OpenAPI v3 schemas for the CRDs. This is where the controller-gen tool from the sigs.kubernetes.io/controller-tools project will come in handy. We also want to generate some stub code for a new reconciling controller into the primary aws-service-operator binary. When we run make generate $SERVICE $VERSION , we should end up with a directory structure like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 /cmd /aws-service-operator main.go /crds /$SERVICE crds.yaml rbac.yaml /pkg /apis /$SERVICE /$VERSION doc.go register.go types.go deepcopy.go defaults.go /client /clientset /versioned /fake ... /scheme ... /typed /$SERVICE /$VERSION \u2026 /controllers /$SERVICE controller.go /informers /externalversions /$SERVICE /$VERSION ... /internalinterfaces /listers /$SERVICE /$VERSION ...","title":"Code generation"},{"location":"dev-docs/code-generation/#code-generation","text":"In order to keep the code for all the service controllers consistent, we will use a strategy of generating the custom resource definitions and controller code stubs for new AWS services.","title":"Code generation"},{"location":"dev-docs/code-generation/#options","text":"To generate the CRDs and controller stub code, we investigated a number of options: home-grown custom code generator kudo kubebuilder a hybrid custom code generator + sigs.kubernetes.io/controller-tools (CR) The original AWS Service Operator used a custom-built generator that processed YAML manifests describing the AWS service and used templates to generate CRDs , the controller code itself and the Go types that represent the CRDs in memory. It's worth noting that the CRDs and the controller code that was generated by the original ASO was very tightly coupled to CloudFormation. In fact, the CRDs for individual AWS services like S3 or RDS were thin wrappers around CloudFormation stacks that described the object being operated upon. kudo is a platform for building Kubernetes Operators. It stores state in its own kudo.dev CRDs and allows users to define \"plans\" for a deployed application to deploy itself. We determined that kudo was not a particularly good fit for ASO for a couple reasons. First, we needed a way to generate CRDs in several API groups (s3.aws.com and iam.aws.com for example) and the ASO controller code isn't deploying an \"application\" that needs to have a controlled deployment plan. Instead, ASO is a controller that facilitates creation and management of various AWS service objects using Kubernetes CRD instances. kubebuilder is the recommended upstream tool for generating CRDs and controller stub code. It is a Go binary that creates the scaffolding for CRDs and controller Go code. It has support for multiple API groups (e.g. s3.amazonaws.com and dynamodb.amazonaws.com ) in a single code repository, so allows for sensible separation of code. Our final option was to build a hybrid custom code generator that used controller-runtime under the hood but allowed us to generate controller stub code for multiple API groups and place generated code in directories that represented Go best practices. This option gives us the flexibility to generate the files and content for multiple API groups but still stay within the recommended guardrails of the upstream Kubernetes community.","title":"Options"},{"location":"dev-docs/code-generation/#hybrid-customcontroller-runtime","text":"This approach uses multiple phases of code generation. The first code generation phase consumes model information from a canonical source of truth about an AWS service and the objects and interfaces that service exposes and generates one or more files containing code that exposes Go types for those objects. These \"type files\" should be annotated with the marker and comments that will allow the core code generators and controller-gen to do its work. Once we have generated the type files, we need to generate basic scaffolding for consumers of those types. The upstream code-generator project contains generators for this scaffolding. We should be able to make a modified version of the upstream generate-groups.sh script to generate all the defaults, deepcopy, informers, listers and clientset code. Next, we will need to generate some skeleton code for the reconciling controller handling the new API along with the YAML files representing the CRDs that will get loaded into kube-apiserver and the YAML files for RBAC and OpenAPI v3 schemas for the CRDs. This is where the controller-gen tool from the sigs.kubernetes.io/controller-tools project will come in handy. We also want to generate some stub code for a new reconciling controller into the primary aws-service-operator binary. When we run make generate $SERVICE $VERSION , we should end up with a directory structure like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 /cmd /aws-service-operator main.go /crds /$SERVICE crds.yaml rbac.yaml /pkg /apis /$SERVICE /$VERSION doc.go register.go types.go deepcopy.go defaults.go /client /clientset /versioned /fake ... /scheme ... /typed /$SERVICE /$VERSION \u2026 /controllers /$SERVICE controller.go /informers /externalversions /$SERVICE /$VERSION ... /internalinterfaces /listers /$SERVICE /$VERSION ...","title":"Hybrid custom+controller-runtime"},{"location":"dev-docs/overview/","text":"Overview \u00b6 TODO","title":"Overview"},{"location":"dev-docs/overview/#overview","text":"TODO","title":"Overview"},{"location":"dev-docs/setup/","text":"Setup \u00b6 In the following, we walk you through the setup to start developing on AWS Controller for Kubernetes (ACK) in the mvp phase. Fork the repository \u00b6 First, fork the upstream source repository to your private GitHub account. Then, on your workstation, run: 1 2 3 4 cd go/src/github.com/aws git clone git@github.com: $GITHUB_ID /aws-controllers-k8s cd aws-controllers-k8s git remote add upstream git@github.com:aws/aws-controllers-k8s Create your local branch \u00b6 1 git fetch --all && git checkout -b $BRANCH_NAME upstream/mvp Commit changes \u00b6 Make your changes locally, commit and push using: 1 2 git add . && git commit git push origin $BRANCH_NAME With an example output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Enumerating objects: 6 , done . Counting objects: 100 % ( 6 /6 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 4 /4 ) , done . Writing objects: 100 % ( 4 /4 ) , 710 bytes | 710 .00 KiB/s, done . Total 4 ( delta 2 ) , reused 0 ( delta 0 ) remote: Resolving deltas: 100 % ( 2 /2 ) , completed with 2 local objects. remote: This repository moved. Please use the new location: remote: git@github.com: $GITHUB_ID /aws-controllers-k8s.git remote: remote: Create a pull request for 'docs' on GitHub by visiting: remote: https://github.com/ $GITHUB_ID /aws-controllers-k8s/pull/new/docs remote: To github.com:a-hilaly/aws-controllers-k8s * [ new branch ] docs -> docs Create pull request \u00b6 Finally, submit a pull request against the upstream source repository. Use either the link that show up as in the example above or to the upstream source repository and open the Pull Request. You'll see a link like the image below:","title":"Setup"},{"location":"dev-docs/setup/#setup","text":"In the following, we walk you through the setup to start developing on AWS Controller for Kubernetes (ACK) in the mvp phase.","title":"Setup"},{"location":"dev-docs/setup/#fork-the-repository","text":"First, fork the upstream source repository to your private GitHub account. Then, on your workstation, run: 1 2 3 4 cd go/src/github.com/aws git clone git@github.com: $GITHUB_ID /aws-controllers-k8s cd aws-controllers-k8s git remote add upstream git@github.com:aws/aws-controllers-k8s","title":"Fork the repository"},{"location":"dev-docs/setup/#create-your-local-branch","text":"1 git fetch --all && git checkout -b $BRANCH_NAME upstream/mvp","title":"Create your local branch"},{"location":"dev-docs/setup/#commit-changes","text":"Make your changes locally, commit and push using: 1 2 git add . && git commit git push origin $BRANCH_NAME With an example output: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Enumerating objects: 6 , done . Counting objects: 100 % ( 6 /6 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 4 /4 ) , done . Writing objects: 100 % ( 4 /4 ) , 710 bytes | 710 .00 KiB/s, done . Total 4 ( delta 2 ) , reused 0 ( delta 0 ) remote: Resolving deltas: 100 % ( 2 /2 ) , completed with 2 local objects. remote: This repository moved. Please use the new location: remote: git@github.com: $GITHUB_ID /aws-controllers-k8s.git remote: remote: Create a pull request for 'docs' on GitHub by visiting: remote: https://github.com/ $GITHUB_ID /aws-controllers-k8s/pull/new/docs remote: To github.com:a-hilaly/aws-controllers-k8s * [ new branch ] docs -> docs","title":"Commit changes"},{"location":"dev-docs/setup/#create-pull-request","text":"Finally, submit a pull request against the upstream source repository. Use either the link that show up as in the example above or to the upstream source repository and open the Pull Request. You'll see a link like the image below:","title":"Create pull request"},{"location":"dev-docs/testing/","text":"Testing \u00b6 NOTE : Testing is tracked in the umbrella Issue 6 . For local development and/or testing we use kind . To build and test an ACK controller against a KinD cluster, execute the following from the root directory of your checked-out source repository: 1 2 3 4 5 make build-ack-generate # Replace with the service you want to build and test an ACK controller for... SERVICE_TO_BUILD=\"ecr\" ./scripts/build-controller.sh $SERVICE_TO_BUILD ./scripts/kind-build-test.sh -s $SERVICE_TO_BUILD The above does the following: Builds the latest ack-generate binary Generates the ACK service controller for the AWS ECR API and output the generated code to the services/$SERVICE_TO_BUILD directory Generates the custom resource definition (CRD) manifests for resources managed by that ACK service controller Generates the Helm chart that can be used to install those CRD manifests and a Deployment manifest that runs the ACK service controller in a Pod on a Kubernetes cluster (still TODO) Provisions a KinD Kubernetes cluster Builds a Docker image containing the ACK service controller Loads the Docker image for the ACK service controller into the KinD cluster Installs the ACK service controller and related Kubernetes manifests into the KinD cluster using kustomize build | kubectl apply -f - Runs a series of Bash test scripts that call kubectl and the aws CLI tools to verify that custom resources (CRs) of the type managed by the ACK service controller are created, updated and deleted appropriately (still TODO) Deletes the Kubernetes cluster created by KinD. You can prevent this last step from happening by passing the -p (for \"preserve\") flag to the scripts/kind-build-test.sh script IMPORTANT NOTE : The first time you run the scripts/kind-build-test.sh script, the step that builds the Docker image for the target ACK service controller can take a LONG time (40+ minutes). This is because a Docker image layer contains a lot of dependencies. Once you successfully build the target Docker image, that base Docker image layer is cached by Docker and the build takes a much shorter amount of time. Cleaning up test runs \u00b6 If you run scripts/kind-build-test.sh with the -p (for \"preserve\") flag, the Kubernetes cluster created by KinD is not destroyed at the end of the test. To cleanup a Kubernetes cluster created by KinD (which will include all the configuration files created by the script specifically for your test cluster), call kind delete cluster --name $CLUSTER_NAME","title":"Testing"},{"location":"dev-docs/testing/#testing","text":"NOTE : Testing is tracked in the umbrella Issue 6 . For local development and/or testing we use kind . To build and test an ACK controller against a KinD cluster, execute the following from the root directory of your checked-out source repository: 1 2 3 4 5 make build-ack-generate # Replace with the service you want to build and test an ACK controller for... SERVICE_TO_BUILD=\"ecr\" ./scripts/build-controller.sh $SERVICE_TO_BUILD ./scripts/kind-build-test.sh -s $SERVICE_TO_BUILD The above does the following: Builds the latest ack-generate binary Generates the ACK service controller for the AWS ECR API and output the generated code to the services/$SERVICE_TO_BUILD directory Generates the custom resource definition (CRD) manifests for resources managed by that ACK service controller Generates the Helm chart that can be used to install those CRD manifests and a Deployment manifest that runs the ACK service controller in a Pod on a Kubernetes cluster (still TODO) Provisions a KinD Kubernetes cluster Builds a Docker image containing the ACK service controller Loads the Docker image for the ACK service controller into the KinD cluster Installs the ACK service controller and related Kubernetes manifests into the KinD cluster using kustomize build | kubectl apply -f - Runs a series of Bash test scripts that call kubectl and the aws CLI tools to verify that custom resources (CRs) of the type managed by the ACK service controller are created, updated and deleted appropriately (still TODO) Deletes the Kubernetes cluster created by KinD. You can prevent this last step from happening by passing the -p (for \"preserve\") flag to the scripts/kind-build-test.sh script IMPORTANT NOTE : The first time you run the scripts/kind-build-test.sh script, the step that builds the Docker image for the target ACK service controller can take a LONG time (40+ minutes). This is because a Docker image layer contains a lot of dependencies. Once you successfully build the target Docker image, that base Docker image layer is cached by Docker and the build takes a much shorter amount of time.","title":"Testing"},{"location":"dev-docs/testing/#cleaning-up-test-runs","text":"If you run scripts/kind-build-test.sh with the -p (for \"preserve\") flag, the Kubernetes cluster created by KinD is not destroyed at the end of the test. To cleanup a Kubernetes cluster created by KinD (which will include all the configuration files created by the script specifically for your test cluster), call kind delete cluster --name $CLUSTER_NAME","title":"Cleaning up test runs"},{"location":"user-docs/install/","text":"Install \u00b6 In the following we walk you through installing an AWS service controller. Helm (recommended) \u00b6 The recommended way to install an AWS service controller for Kubernetes is to use Helm 3. Before installing an AWS service controller, ensure you have added the AWS Controllers for Kubernetes Helm repository: 1 helm repo add ack https://aws.github.io/aws-service-operator-k8s Each AWS service controller is packaged into a separate container image, published on a public AWS Elastic Container Registry repository. Likewise, each AWS service controller has a separate Helm chart that installs\u2014as a Kubernetes Deployment \u2014the AWS service controller, necessary custom resource definitions (CRDs), Kubernetes RBAC manifests, and other supporting artifacts. You may install a particular AWS service controller using the helm install CLI command: 1 helm install [--namespace $KUBERNETES_NAMESPACE] ack/$SERVICE_ALIAS for example, if you wanted to install the AWS S3 service controller into the \"ack-system\" Kubernetes namespace, you would execute: 1 helm install --namespace ack-system ack/s3 Static Kubernetes manifests \u00b6 If you prefer not to use Helm, you may install a service controller using static Kubernetes manifests. Static Kubernetes manifests that install individual service controllers are attached as artifacts to releases of AWS Controllers for Kubernetes. Select a release from the list of releases for AWS Controllers for Kubernetes. You will see a list of Assets for the release. One of those Assets will be named services/$SERVICE_ALIAS/all-resources.yaml . For example, for the AWS S3 service controller, there will be an Asset named services/s3/all-resources.yaml attached to the release. Click on the link to download the YAML file. This YAML file may be fed to kubectl apply -f directly to install the service controller, any CRDs that it manages, and all necessary Kubernetes RBAC manifests. For example: 1 kubectl apply -f https://github.com/aws/aws-controllers-k8s/releases/download/v0.0.1/services/s3/all-resources.yaml Once you've installed one or more ACKs, make sure to configure permissions , next.","title":"Install"},{"location":"user-docs/install/#install","text":"In the following we walk you through installing an AWS service controller.","title":"Install"},{"location":"user-docs/install/#helm-recommended","text":"The recommended way to install an AWS service controller for Kubernetes is to use Helm 3. Before installing an AWS service controller, ensure you have added the AWS Controllers for Kubernetes Helm repository: 1 helm repo add ack https://aws.github.io/aws-service-operator-k8s Each AWS service controller is packaged into a separate container image, published on a public AWS Elastic Container Registry repository. Likewise, each AWS service controller has a separate Helm chart that installs\u2014as a Kubernetes Deployment \u2014the AWS service controller, necessary custom resource definitions (CRDs), Kubernetes RBAC manifests, and other supporting artifacts. You may install a particular AWS service controller using the helm install CLI command: 1 helm install [--namespace $KUBERNETES_NAMESPACE] ack/$SERVICE_ALIAS for example, if you wanted to install the AWS S3 service controller into the \"ack-system\" Kubernetes namespace, you would execute: 1 helm install --namespace ack-system ack/s3","title":"Helm (recommended)"},{"location":"user-docs/install/#static-kubernetes-manifests","text":"If you prefer not to use Helm, you may install a service controller using static Kubernetes manifests. Static Kubernetes manifests that install individual service controllers are attached as artifacts to releases of AWS Controllers for Kubernetes. Select a release from the list of releases for AWS Controllers for Kubernetes. You will see a list of Assets for the release. One of those Assets will be named services/$SERVICE_ALIAS/all-resources.yaml . For example, for the AWS S3 service controller, there will be an Asset named services/s3/all-resources.yaml attached to the release. Click on the link to download the YAML file. This YAML file may be fed to kubectl apply -f directly to install the service controller, any CRDs that it manages, and all necessary Kubernetes RBAC manifests. For example: 1 kubectl apply -f https://github.com/aws/aws-controllers-k8s/releases/download/v0.0.1/services/s3/all-resources.yaml Once you've installed one or more ACKs, make sure to configure permissions , next.","title":"Static Kubernetes manifests"},{"location":"user-docs/permissions/","text":"Configure permissions \u00b6 Because ACK bridges the Kubernetes and AWS APIs, before using ACK service controllers, you will need to do some initial configuration around Kubernetes and AWS Identity and Access Management (IAM) permissions. Configuring Kubernetes RBAC \u00b6 As part of installation, certain Kubernetes Role objects will be created that contain permissions to modify the Kubernetes custom resource (CR) objects that the ACK service controller is responsible for. NOTE : All Kubernetes CR objects managed by an ACK service controller are Namespaced objects; that is, there are no cluster-scoped ACK-managed CRs. By default, the following Kubernetes Role objects are created when installing an ACK service controller: ack.user : a Role used for reading and mutating namespace-scoped custom resource (CR) objects that the service controller manages. ack.reader : a Role used for reading namespaced-scoped custom resource (CR) objects that the service controller manages. When installing a service controller, if the Role already exists (because an ACK controller for a different AWS service has previously been installed), permissions to manage CRD and CR objects associated with the installed controller's AWS service are added to the existing Role . For example, if you installed the ACK service controller for AWS S3, during that installation process, the ack.user Role would have been granted read/write permissions to create CRs with a GroupKind of s3.services.k8s.aws/Bucket within a specific Kubernetes Namespace . Likewise the ack.reader Role would be been granted read permissions to view CRs with a GroupKind of s3.services.k8s.aws . If you later installed the ACK service controller for AWS SNS, the installation process would have added permissions to the ack.user Role to read/write CR objects of GroupKind sns.services.k8s.aws/Topic and added permissions to the ack.user Role to read CR objects of GroupKind sns.services.k8s.aws/Topic . If you would like to use a differently-named Kubernetes Role than the defaults, you are welcome to do so by modifying the Kubernetes manifests that are used as part of the installation process. Once the Kubernetes Role objects have been created, you will want to assign specific a Kubernetes User to a particular Role . You do this using the typical Kubernetes RoleBinding object. For example, assume you want to have the Kubernetes User named \"Alice\" have the ability to create, read, delete and modify CRs that ACK service controllers manage in the Kubernetes \"default\" Namespace , you would create a RoleBinding that looked like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : ack.user namespace : default subjects : - kind : User name : Alice apiGroup : rbac.authorization.k8s.io roleRef : kind : Role name : ack.user apiGroup : rbac.authorization.k8s.io Configuring AWS IAM \u00b6 Since ACK service controllers bridge the Kubernetes and AWS API worlds, in addition to configuring Kubernetes RBAC permissions, you will need to ensure that all AWS Identity and Access Management (IAM) roles and permissions have been properly created. TODO Cross-account resource management \u00b6 TODO","title":"Permissions"},{"location":"user-docs/permissions/#configure-permissions","text":"Because ACK bridges the Kubernetes and AWS APIs, before using ACK service controllers, you will need to do some initial configuration around Kubernetes and AWS Identity and Access Management (IAM) permissions.","title":"Configure permissions"},{"location":"user-docs/permissions/#configuring-kubernetes-rbac","text":"As part of installation, certain Kubernetes Role objects will be created that contain permissions to modify the Kubernetes custom resource (CR) objects that the ACK service controller is responsible for. NOTE : All Kubernetes CR objects managed by an ACK service controller are Namespaced objects; that is, there are no cluster-scoped ACK-managed CRs. By default, the following Kubernetes Role objects are created when installing an ACK service controller: ack.user : a Role used for reading and mutating namespace-scoped custom resource (CR) objects that the service controller manages. ack.reader : a Role used for reading namespaced-scoped custom resource (CR) objects that the service controller manages. When installing a service controller, if the Role already exists (because an ACK controller for a different AWS service has previously been installed), permissions to manage CRD and CR objects associated with the installed controller's AWS service are added to the existing Role . For example, if you installed the ACK service controller for AWS S3, during that installation process, the ack.user Role would have been granted read/write permissions to create CRs with a GroupKind of s3.services.k8s.aws/Bucket within a specific Kubernetes Namespace . Likewise the ack.reader Role would be been granted read permissions to view CRs with a GroupKind of s3.services.k8s.aws . If you later installed the ACK service controller for AWS SNS, the installation process would have added permissions to the ack.user Role to read/write CR objects of GroupKind sns.services.k8s.aws/Topic and added permissions to the ack.user Role to read CR objects of GroupKind sns.services.k8s.aws/Topic . If you would like to use a differently-named Kubernetes Role than the defaults, you are welcome to do so by modifying the Kubernetes manifests that are used as part of the installation process. Once the Kubernetes Role objects have been created, you will want to assign specific a Kubernetes User to a particular Role . You do this using the typical Kubernetes RoleBinding object. For example, assume you want to have the Kubernetes User named \"Alice\" have the ability to create, read, delete and modify CRs that ACK service controllers manage in the Kubernetes \"default\" Namespace , you would create a RoleBinding that looked like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : ack.user namespace : default subjects : - kind : User name : Alice apiGroup : rbac.authorization.k8s.io roleRef : kind : Role name : ack.user apiGroup : rbac.authorization.k8s.io","title":"Configuring Kubernetes RBAC"},{"location":"user-docs/permissions/#configuring-aws-iam","text":"Since ACK service controllers bridge the Kubernetes and AWS API worlds, in addition to configuring Kubernetes RBAC permissions, you will need to ensure that all AWS Identity and Access Management (IAM) roles and permissions have been properly created. TODO","title":"Configuring AWS IAM"},{"location":"user-docs/permissions/#cross-account-resource-management","text":"TODO","title":"Cross-account resource management"},{"location":"user-docs/usage/","text":"Usage \u00b6 In this section we discuss how to use AWS Controllers for Kubernetes. Prerequisites \u00b6 Before using AWS Controllers for Kubernetes, make sure to: install one or more ACK service controllers. configure permissions of Kubernetes and AWS Identity and Access Management Roles. Creating an AWS resource via the Kubernetes API \u00b6 TODO Viewing AWS resource information via the Kubernetes API \u00b6 TODO Deleting an AWS resource via the Kubernetes API \u00b6 TODO Modifying an AWS resource via the Kubernetes API \u00b6 TODO","title":"Usage"},{"location":"user-docs/usage/#usage","text":"In this section we discuss how to use AWS Controllers for Kubernetes.","title":"Usage"},{"location":"user-docs/usage/#prerequisites","text":"Before using AWS Controllers for Kubernetes, make sure to: install one or more ACK service controllers. configure permissions of Kubernetes and AWS Identity and Access Management Roles.","title":"Prerequisites"},{"location":"user-docs/usage/#creating-an-aws-resource-via-the-kubernetes-api","text":"TODO","title":"Creating an AWS resource via the Kubernetes API"},{"location":"user-docs/usage/#viewing-aws-resource-information-via-the-kubernetes-api","text":"TODO","title":"Viewing AWS resource information via the Kubernetes API"},{"location":"user-docs/usage/#deleting-an-aws-resource-via-the-kubernetes-api","text":"TODO","title":"Deleting an AWS resource via the Kubernetes API"},{"location":"user-docs/usage/#modifying-an-aws-resource-via-the-kubernetes-api","text":"TODO","title":"Modifying an AWS resource via the Kubernetes API"}]}